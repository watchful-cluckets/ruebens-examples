{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from subprocess import call\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import glob\n",
    "import re\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import operator\n",
    "import collections\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import theano as thno\n",
    "import xgboost as xgb\n",
    "import keras as krs\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import grid_search\n",
    "from sklearn import pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"dark\", palette=\"muted\")\n",
    "sns.set_context(\"notebook\",\n",
    "                font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "rseed = random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ken Cavagnolo \n",
      "last updated: Mon Aug 22 2016 \n",
      "\n",
      "CPython 2.7.10\n",
      "IPython 4.1.1\n",
      "\n",
      "numpy 1.11.1\n",
      "scipy 0.18.0\n",
      "pandas 0.17.0\n",
      "seaborn 0.8.dev0\n",
      "plotly 1.9.0\n",
      "scikit-learn 0.17.1\n",
      "xgboost 0.6a2\n",
      "keras 1.0.4\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.72)\n",
      "system     : Darwin\n",
      "release    : 15.6.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : DrGonzo.local\n",
      "Git hash   : 4cfc9a42cf161e6d57a2e18ad9f942d3b78b61a8\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Ken Cavagnolo\" -n -u -v -m -h -g -p numpy,scipy,pandas,seaborn,plotly,scikit-learn,xgboost,keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(y_pred, y_true):\n",
    "    nrows = y_pred.shape[0]\n",
    "    y_pred = y_pred + 1e-15\n",
    "    probs = y_pred[list(range(nrows)), y_true.astype(int)]\n",
    "    return -np.sum(np.log(probs)) / nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_report(grid_scores, n_top=5):\n",
    "    top_scores = sorted(grid_scores, key=operator.itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model #{0}\".format(i + 1))\n",
    "        print(\"Mean CV score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Est. Kscore: {:.3f}\".format(1/score.mean_validation_score))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factors(n):    \n",
    "    return set(reduce(list.__add__,\n",
    "                      ([i, n//i] for i in range(1, int(n**0.5) + 1) if n % i == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_indices(df, attr):\n",
    "    idx = {}\n",
    "    for a in attr:\n",
    "        idx[a] = [c for c in df.columns if c.startswith(a)]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_combos(attr):\n",
    "    combos = []\n",
    "    for i in range(1, len(attr)+1):\n",
    "        for subset in itertools.combinations(attr, i):\n",
    "            combos.append(subset)\n",
    "    return combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_mixer(df, attr, clf_class, **kwargs):\n",
    "    \n",
    "    # scoring arrays\n",
    "    acc = []\n",
    "    prec = []\n",
    "    recl = []\n",
    "    f1 = []\n",
    "    ks = []\n",
    "    \n",
    "    # get attribute combos to explore\n",
    "    attr_com = all_combos(attr)\n",
    "\n",
    "    # get these attr as indices\n",
    "    attr_idx = feature_indices(df, attr)\n",
    "    \n",
    "    # labelizer\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    \n",
    "    # iterate through all combinations of features\n",
    "    for a in attr_com:\n",
    "\n",
    "        # get feature indices\n",
    "        idxs = []\n",
    "        for b in a:\n",
    "            idxs.extend(attr_idx[b])\n",
    "\n",
    "        # build datasets    \n",
    "        X = df.ix[:, (idxs)].copy().values\n",
    "        y = df.fault_severity.values\n",
    "        y_cat = lb.fit_transform(y)\n",
    "        y_pred = y.copy()\n",
    "        y_prob = y_cat.copy().astype(float)\n",
    "        \n",
    "        # initialize counters\n",
    "        mean_acc = 0.0\n",
    "        mean_prec = 0.0\n",
    "        mean_recl = 0.0\n",
    "        mean_f1 = 0.0\n",
    "        mean_ks = 0.0\n",
    "        \n",
    "        # run kfold cv\n",
    "        kf = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "        for train_index, test_index in kf:\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clf = clf_class(**kwargs)\n",
    "            clf = pipeline.Pipeline([('scaler', preprocessing.StandardScaler()), ('clf', clf)])\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred[test_index] = clf.predict(X_test)\n",
    "            y_prob[test_index] = clf.predict_proba(X_test)\n",
    "            mean_acc += metrics.accuracy_score(y[test_index], y_pred[test_index])\n",
    "            mean_recl += metrics.recall_score(y[test_index], y_pred[test_index])\n",
    "            mean_prec += metrics.precision_score(y[test_index], y_pred[test_index])\n",
    "            mean_f1 += metrics.f1_score(y[test_index], y_pred[test_index])\n",
    "            mean_ks += kscore(y_prob[test_index], y[test_index])\n",
    "\n",
    "        # update mean counters\n",
    "        acc.append(mean_acc / len(kf))\n",
    "        recl.append(mean_recl / len(kf))\n",
    "        prec.append(mean_prec / len(kf))\n",
    "        f1.append(mean_f1 / len(kf))\n",
    "        ks.append(mean_ks / len(kf))\n",
    "\n",
    "    # make dataframe\n",
    "    result = pd.DataFrame({'attr_combo':attr_com, 'accuracy':acc,\n",
    "                           'precision':prec, 'recall':recl, 'f1':f1,\n",
    "                           'kscore':ks})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyXGBClassifier(object):\n",
    "\n",
    "    def __init__(self, n_rounds=100, **params):\n",
    "        self.clf = None\n",
    "        self.params = params        \n",
    "        self.n_rounds = n_rounds\n",
    "        self.dtrain = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        num_boost_round = self.n_rounds\n",
    "        self.dtrain = xgb.DMatrix(X, label=Y)\n",
    "        self.clf = xgb.train(params=self.params,\n",
    "                             dtrain=self.dtrain,\n",
    "                             num_boost_round=num_boost_round,\n",
    "                             verbose_eval=False)\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y = self.clf.predict_proba(X)\n",
    "        y = np.argmax(Y, axis=1)\n",
    "        return np.array(y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        return self.clf.predict(dtest)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.params.update(params)\n",
    "        return self\n",
    "\n",
    "    def logloss(self, X, Y):\n",
    "        return logloss(self, X, Y)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        y = self.predict_proba(X)\n",
    "        return 1. / logloss(y, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(grid_scores, n_top):\n",
    "\n",
    "    cols_key = []\n",
    "    top_scores = sorted(grid_scores, key=operator.itemgetter(1), reverse=True)[:n_top]\n",
    "\n",
    "    for i, score in enumerate(top_scores):\n",
    "        if( i < 5):\n",
    "            print(\"Model with rank: {0}\".format(i + 1))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "            print(\"Parameters: {0}\".format(score.parameters))\n",
    "            print(\"\")\n",
    "\n",
    "        dict1 = collections.OrderedDict(sorted(score.parameters.items()))\n",
    "\n",
    "        if i==0:\n",
    "            for key in dict1.keys():\n",
    "                cols_key.append(key)\n",
    "            Parms_DF =  pd.DataFrame(columns=cols_key)\n",
    "\n",
    "        cols_val = []\n",
    "        for key in dict1.keys():\n",
    "            cols_val.append(dict1[key])\n",
    "\n",
    "        Parms_DF.loc[i] =  cols_val\n",
    "\n",
    "    return Parms_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_csv(\"features.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_number</th>\n",
       "      <th>invoice_amount</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>invoice_dunning_proc</th>\n",
       "      <th>invoice_dodaac</th>\n",
       "      <th>invoice_terms_of_payment_key</th>\n",
       "      <th>payment_posting_date_in_the_document</th>\n",
       "      <th>ar_age_(days)</th>\n",
       "      <th>ar_age_label</th>\n",
       "      <th>ar_days_late</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_std_day_of_paid_invoices</th>\n",
       "      <th>feature_number_of_paid_invoices_over_number_of_invoices</th>\n",
       "      <th>feature_number_of_invoices_paid_late_over_number_of_invoices</th>\n",
       "      <th>feature_sum_of_amount_of_paid_invoices_over_sum_of_amount_of_invoices</th>\n",
       "      <th>feature_sum_of_amount_of_invoices_paid_late_over_sum_of_amount_of_invoices</th>\n",
       "      <th>feature_number_of_outstanding_invoices_over_number_of_invoices</th>\n",
       "      <th>feature_sum_of_amount_of_outstanding_invoices_over_sum_of_amount_of_invoices</th>\n",
       "      <th>feature_ratio_of_invoices_paid_late</th>\n",
       "      <th>feature_ratio_of_sum_of_paid_amount_late</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000961</td>\n",
       "      <td>178.60</td>\n",
       "      <td>2002-09-16</td>\n",
       "      <td>PKTR</td>\n",
       "      <td>EZ8342</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-10-19</td>\n",
       "      <td>2590</td>\n",
       "      <td>91+</td>\n",
       "      <td>2590</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001073</td>\n",
       "      <td>19.80</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>PKTR</td>\n",
       "      <td>EZ9667</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-06-24</td>\n",
       "      <td>2970</td>\n",
       "      <td>91+</td>\n",
       "      <td>2970</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000001073</td>\n",
       "      <td>8.80</td>\n",
       "      <td>2010-07-31</td>\n",
       "      <td>PKTR</td>\n",
       "      <td>EZ9667</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-08-27</td>\n",
       "      <td>27</td>\n",
       "      <td>1-30</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000385763</td>\n",
       "      <td>34.00</td>\n",
       "      <td>2004-01-29</td>\n",
       "      <td>PKTR</td>\n",
       "      <td>L00196</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-06-10</td>\n",
       "      <td>2689</td>\n",
       "      <td>91+</td>\n",
       "      <td>2689</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000385763</td>\n",
       "      <td>3704.78</td>\n",
       "      <td>2004-02-10</td>\n",
       "      <td>PKTR</td>\n",
       "      <td>L00196</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>2661</td>\n",
       "      <td>91+</td>\n",
       "      <td>2661</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_number  invoice_amount invoice_date invoice_dunning_proc  \\\n",
       "0       1000000961          178.60   2002-09-16                 PKTR   \n",
       "1       1000001073           19.80   2003-05-07                 PKTR   \n",
       "2       1000001073            8.80   2010-07-31                 PKTR   \n",
       "3       1000385763           34.00   2004-01-29                 PKTR   \n",
       "4       1000385763         3704.78   2004-02-10                 PKTR   \n",
       "\n",
       "  invoice_dodaac invoice_terms_of_payment_key  \\\n",
       "0         EZ8342                            1   \n",
       "1         EZ9667                            1   \n",
       "2         EZ9667                            1   \n",
       "3         L00196                            1   \n",
       "4         L00196                            1   \n",
       "\n",
       "  payment_posting_date_in_the_document  ar_age_(days) ar_age_label  \\\n",
       "0                           2009-10-19           2590          91+   \n",
       "1                           2011-06-24           2970          91+   \n",
       "2                           2010-08-27             27         1-30   \n",
       "3                           2011-06-10           2689          91+   \n",
       "4                           2011-05-25           2661          91+   \n",
       "\n",
       "   ar_days_late    ...     feature_std_day_of_paid_invoices  \\\n",
       "0          2590    ...                                -9999   \n",
       "1          2970    ...                                -9999   \n",
       "2            27    ...                                -9999   \n",
       "3          2689    ...                                -9999   \n",
       "4          2661    ...                                -9999   \n",
       "\n",
       "   feature_number_of_paid_invoices_over_number_of_invoices  \\\n",
       "0                                                  0         \n",
       "1                                                  0         \n",
       "2                                                  0         \n",
       "3                                                  0         \n",
       "4                                                  0         \n",
       "\n",
       "   feature_number_of_invoices_paid_late_over_number_of_invoices  \\\n",
       "0                                                  0              \n",
       "1                                                  0              \n",
       "2                                                  0              \n",
       "3                                                  0              \n",
       "4                                                  0              \n",
       "\n",
       "   feature_sum_of_amount_of_paid_invoices_over_sum_of_amount_of_invoices  \\\n",
       "0                                                  0                       \n",
       "1                                                  0                       \n",
       "2                                                  0                       \n",
       "3                                                  0                       \n",
       "4                                                  0                       \n",
       "\n",
       "   feature_sum_of_amount_of_invoices_paid_late_over_sum_of_amount_of_invoices  \\\n",
       "0                                                  0                            \n",
       "1                                                  0                            \n",
       "2                                                  0                            \n",
       "3                                                  0                            \n",
       "4                                                  0                            \n",
       "\n",
       "   feature_number_of_outstanding_invoices_over_number_of_invoices  \\\n",
       "0                                                  1                \n",
       "1                                                  1                \n",
       "2                                                  1                \n",
       "3                                                  1                \n",
       "4                                                  1                \n",
       "\n",
       "   feature_sum_of_amount_of_outstanding_invoices_over_sum_of_amount_of_invoices  \\\n",
       "0                                                  1                              \n",
       "1                                                  1                              \n",
       "2                                                  1                              \n",
       "3                                                  1                              \n",
       "4                                                  1                              \n",
       "\n",
       "   feature_ratio_of_invoices_paid_late  \\\n",
       "0                                -9999   \n",
       "1                                -9999   \n",
       "2                                -9999   \n",
       "3                                -9999   \n",
       "4                                -9999   \n",
       "\n",
       "   feature_ratio_of_sum_of_paid_amount_late  is_train  \n",
       "0                                     -9999      True  \n",
       "1                                     -9999     False  \n",
       "2                                     -9999      True  \n",
       "3                                     -9999      True  \n",
       "4                                     -9999      True  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [c.lower().replace(' ', '_').replace('?', '').replace(\"'\", \"\").replace(\"#\",\"num\") for c in df.columns]\n",
    "df['is_train'] = np.random.uniform(False, True, len(df)) <= 0.5\n",
    "df['ar_age_label'] = df['ar_age_label'].str.replace(\" days late\", \"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoice_date\n",
      "payment_posting_date_in_the_document\n"
     ]
    }
   ],
   "source": [
    "for c in df.columns:\n",
    "    if 'date' in c:\n",
    "        print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['customer_number'] != 8050000993]\n",
    "df = df.loc[df['customer_number'] != 3000020663]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lbin = preprocessing.LabelEncoder()\n",
    "df.ar_age_label = lbin.fit_transform(df.ar_age_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['invoice_dunning_proc', 'invoice_terms_of_payment_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_cols = ['customer_number',\n",
    "             'invoice_date',\n",
    "             'payment_posting_date_in_the_document',\n",
    "             'invoice_dodaac',\n",
    "             'ar_age_(days)',\n",
    "             'ar_age_label',\n",
    "             'ar_days_late',\n",
    "             'is_train',\n",
    "             'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['target'] = df.ar_age_label\n",
    "n_classes = len(np.unique(df.target.values))\n",
    "y_train = df[(df.is_train==True)].target.values\n",
    "y_test = df[(df.is_train==False)].target.values\n",
    "X = df.copy()\n",
    "X_train = X[(X.is_train==True)]\n",
    "X_test = X[(X.is_train==False)]\n",
    "X_train.drop(drop_cols, axis=1, inplace=True)\n",
    "X_test.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# First, Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV as LR\n",
    "clf = LR()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.60\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.59      0.73     17915\n",
      "          1       0.21      0.70      0.32      1565\n",
      "          2       0.00      0.00      0.00         1\n",
      "          3       0.18      0.55      0.27      1014\n",
      "          4       0.20      0.70      0.32        54\n",
      "\n",
      "avg / total       0.86      0.60      0.68     20549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "print metrics.classification_report(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile = open('xgb.fmap', 'w')\n",
    "for i, feat in enumerate(list(train.columns)):\n",
    "    outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "importance = model.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "df_imp = pd.DataFrame(importance[:50], columns=['feature', 'fscore'])\n",
    "df_imp['fscore'] = df_imp['fscore'] / df_imp['fscore'].sum()\n",
    "\n",
    "plt.figure()\n",
    "df_imp.plot()\n",
    "df_imp.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data has complex multi-dimensional structure that ML algos know how to find and exploit to make decisions\n",
    "* What's not exposed to the models?\n",
    "    * Events are too dense or too complex for the algo to find. Fix == dim reduction (PCA or SVD)\n",
    "    * How the various events are communicating. Fix == clustering\n",
    "    * There is more attribute aggregation/creation needed. Fix == hyperattributes\n",
    "    * Time and location dependance of the events. Fix == ???\n",
    "\n",
    "* Remove correlated features\n",
    "* Remove features using statistical tests\n",
    "* Try pair-wise feature interactions, e.g. a*b, a-b, a+b, a/b\n",
    "* Try feature transformations, e.g. sqrt(a), log(a), abs(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "X_train = df_all[(df_all.is_train==True)].copy().drop(drop, 1)\n",
    "xcols   = X_train.columns\n",
    "X_train = X_train.values\n",
    "y_train = df_all[(df_all.is_train==True)].fault_severity.values\n",
    "\n",
    "# simple rand forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=3000, n_jobs=-1)\n",
    "scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=15, n_jobs=-1)\n",
    "print scores.mean(), '+/-', scores.std()\n",
    "\n",
    "# top ten features\n",
    "clf = pipeline.Pipeline([('scaler', preprocessing.StandardScaler()), ('clf', clf)])\n",
    "clf.fit(X_train, y_train)\n",
    "fea_impt = zip(xcols, (clf.feature_importances_ * 100.0).astype(int))\n",
    "sorted(fea_impt, key=lambda x: -x[1])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "X = df_train.copy().drop(drop, 1)\n",
    "cols = X.columns\n",
    "X = X.values\n",
    "y = df_train.fault_severity.values.astype(int)\n",
    "\n",
    "mpl_fig = plt.figure(1, figsize=(11,8))\n",
    "plt.clf()\n",
    "\n",
    "X_indices = np.arange(X.shape[-1])\n",
    "\n",
    "# Univariate feature selection with F-test for feature scoring\n",
    "selector = SelectPercentile(f_classif, percentile=50)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# get scores\n",
    "scores = -np.log10(np.array(selector.pvalues_))\n",
    "scores[np.isnan(scores)] = 0\n",
    "scores /= scores.max()\n",
    "plt.bar(X_indices - .45, scores,\n",
    "        width=.2, label=r'Univariate score ($-Log(p_{value})$)', color='g')\n",
    "\n",
    "# Compare to the weights of an SVM\n",
    "clf = LinearSVC(C=0.3, penalty=\"l1\", dual=False)\n",
    "clf.fit(X, y)\n",
    "\n",
    "svm_weights = (clf.coef_ ** 2).sum(axis=0)\n",
    "svm_weights /= svm_weights.max()\n",
    "\n",
    "plt.bar(X_indices - .25, svm_weights,\n",
    "        width=.2, label='SVM weight', color='r')\n",
    "\n",
    "clf_selected = LinearSVC(C=0.3, penalty=\"l1\", dual=False)\n",
    "clf_selected.fit(selector.transform(X), y)\n",
    "\n",
    "svm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)\n",
    "svm_weights_selected /= svm_weights_selected.max()\n",
    "\n",
    "plt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,\n",
    "        width=.2, label='SVM weights after selection', color='b')\n",
    "\n",
    "\n",
    "plt.xlabel('Supported feature number')\n",
    "plt.yticks(())\n",
    "plt.axis('tight')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print selector.transform(X).shape[1]\n",
    "uni_fea = Xcols[selector.get_support(indices=True)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get index of max value\n",
    "idx = X_indices[selector.get_support()][svm_weights_selected.argmax()]\n",
    "\n",
    "# get column name\n",
    "Xcols[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well no shit, it's log_feature_203 again. Check the baseline models section, it showed up in the decision tree feature importance list as #1 and being almost 2x more important than #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'n_rounds':500,\n",
    "          'eta':0.3,\n",
    "          'max_depth':6,\n",
    "          'subsample':0.5,\n",
    "          'colsample_bytree':0.3,\n",
    "          'booster':'gbtree',\n",
    "          'objective':'multi:softprob',\n",
    "          'num_class':3,\n",
    "          'eval_metric':'mlogloss',\n",
    "          'silent':0}\n",
    "\n",
    "best = []\n",
    "for n in range(70, 105, 5):\n",
    "    ch2 = feature_selection.GenericUnivariateSelect(score_func=feature_selection.chi2,\n",
    "                                  mode='percentile',\n",
    "                                  param=n)\n",
    "\n",
    "    fs_train = ch2.fit_transform(train, labels)\n",
    "    fs_test = ch2.transform(test)\n",
    "    fs_train = xgb.DMatrix(fs_train, labels)\n",
    "    scores = xgb.cv(params, fs_train, num_round, nfold=10,\n",
    "                    metrics={'mlogloss'}, seed=rseed)\n",
    "    best.append((n, scores[(scores['test-mlogloss-mean'] == min(scores['test-mlogloss-mean']))].values[0]))\n",
    "print best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to know what features may be the most important before any transformation. Try recursive feature elimination with cross-validation (RFECV). For multiclass, stratified K-fold used by default and shuffle is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "# get training data\n",
    "X = df_all[(df_all.is_train == True)].copy()\n",
    "y = X.fault_severity\n",
    "X.drop(drop, 1, inplace=True)\n",
    "\n",
    "# make df to store results\n",
    "df_frank = pd.DataFrame({'feature':X.columns.values})\n",
    "\n",
    "# create base classifier\n",
    "models =[]\n",
    "models.append(LR(n_jobs=-1))\n",
    "models.append(RF(n_jobs=-1))\n",
    "models.append(SGD(n_jobs=-1))\n",
    "models.append(MNB())\n",
    "\n",
    "# iterate over models\n",
    "for m in models:\n",
    "    mname = str(m).split('(')[0]\n",
    "    rfecv = RFECV(m, cv=3, scoring='accuracy')\n",
    "    rfecv = rfecv.fit(X, y)\n",
    "    print(\"%s Optimal # features: %d\" % (mname, rfecv.n_features_))\n",
    "    c = 'rfecv_'+str(mname[:3])\n",
    "    df_frank[c] = rfecv.ranking_\n",
    "    \n",
    "    # n_features vs cv score\n",
    "    plt.figure()\n",
    "    plt.title(mname)\n",
    "    plt.xlabel(\"# features\")\n",
    "    plt.ylabel(\"CV score\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.axvline(rfecv.n_features_, color='r', linestyle='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB is not a good model, nor SGD. Try SVC linear in their place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "m = SVC(kernel='linear')\n",
    "mname = str(m).split('(')[0]\n",
    "rfecv = RFECV(m, cv=3, scoring='accuracy')\n",
    "rfecv = rfecv.fit(X, y)\n",
    "print(\"%s Optimal # features: %d\" % (mname, rfecv.n_features_))\n",
    "c = 'rfecv_'+str(mname[:3])\n",
    "df_frank[c] = rfecv.ranking_\n",
    "    \n",
    "# n_features vs cv score\n",
    "plt.figure()\n",
    "plt.title(mname)\n",
    "plt.xlabel(\"# features\")\n",
    "plt.ylabel(\"CV score\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.axvline(rfecv.n_features_, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get avg rank and abs rank\n",
    "df_frank['avg_rank'] = df_frank[['rfecv_Log', 'rfecv_Ran', 'rfecv_SVC']].mean(axis=1)\n",
    "df_frank['abs_rank'] = df_frank.avg_rank.rank(method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_frank', df_frank)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "X = df_train.copy().drop(drop, 1)\n",
    "cols = X.columns\n",
    "y = df_train.fault_severity\n",
    "clf = LinearSVC(C=0.3, penalty=\"l1\", dual=False)\n",
    "selector = feature_selection.SelectFromModel(clf, threshold=0.50)\n",
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print selector.transform(X).shape[1]\n",
    "imp_fea = cols[selector.get_support(indices=True)].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matricies show there are redundant values. How many components in the full dataset to preserve? The eigenvalues in SVD should help determine what attributes are valuable. Data is sparse matrix of binary and multiclass values, options for analysis:\n",
    "* [Single value decomposition and Principal component analysis](http://blog.applied.ai/visualising-high-dimensional-data/)\n",
    "* [Kernel PCA](http://sebastianraschka.com/Articles/2014_kernel_pca.html)\n",
    "* [Locally Linear Embedding](http://scikit-learn.org/stable/auto_examples/manifold/plot_swissroll.html)\n",
    "* [t-Distributed Stochastic Neighbor Embedding](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance preserved at 1 components == 98.5%\n",
      "Variance preserved at 2 components == 99.6%\n",
      "Variance preserved at 3 components == 99.9%\n",
      "Variance preserved at 4 components == 100.0%\n",
      "Variance preserved at 5 components == 100.0%\n",
      "Variance preserved at 6 components == 100.0%\n",
      "Variance preserved at 7 components == 100.0%\n",
      "Variance preserved at 8 components == 100.0%\n",
      "Variance preserved at 9 components == 100.0%\n",
      "Variance preserved at 10 components == 100.0%\n",
      "Variance preserved at 11 components == 100.0%\n",
      "Variance preserved at 12 components == 100.0%\n",
      "Variance preserved at 13 components == 100.0%\n",
      "Variance preserved at 14 components == 100.0%\n",
      "Variance preserved at 15 components == 100.0%\n",
      "Variance preserved at 16 components == 100.0%\n",
      "Variance preserved at 17 components == 100.0%\n",
      "Variance preserved at 18 components == 100.0%\n",
      "Variance preserved at 19 components == 100.0%\n",
      "Variance preserved at 20 components == 100.0%\n",
      "Variance preserved at 21 components == 100.0%\n",
      "Variance preserved at 22 components == 100.0%\n",
      "Variance preserved at 23 components == 100.0%\n",
      "Variance preserved at 24 components == 100.0%\n",
      "Variance preserved at 25 components == 100.0%\n",
      "Variance preserved at 26 components == 100.0%\n",
      "Variance preserved at 27 components == 100.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAFdCAYAAAAT7FToAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ddab910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data\n",
    "Xsvd = X.copy()\n",
    "Xsvd.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "# orig num comps\n",
    "ncomps = Xsvd.shape[1] - 1\n",
    "\n",
    "# run svd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=ncomps)\n",
    "svd_fit = svd.fit(Xsvd)\n",
    "\n",
    "# print out variance ranges\n",
    "for i in range(0, Xsvd.shape[1]-1, 1):\n",
    "    svar = svd_fit.explained_variance_ratio_.cumsum()[i]\n",
    "    if svar >= 0.95:\n",
    "        print('Variance preserved at {:} components == {:.1%}'.format(i, svar))\n",
    "\n",
    "# plot result        \n",
    "ax = pd.Series(svd_fit.explained_variance_ratio_.cumsum()).plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 60 components 98% of variance is preserved, so try keeping just those in a new df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform X\n",
    "ncomps = 4\n",
    "X_svd = TruncatedSVD(n_components=ncomps).fit_transform(X)\n",
    "\n",
    "# save svd to df and add back dropped cols\n",
    "df_svd = pd.DataFrame(X_svd, columns=['svd{}'.format(c) for c in range(ncomps)], index=df.index)\n",
    "\n",
    "# save col names for easy filter later\n",
    "svdcols = [c for c in df_svd.columns if c[:3] == 'svd']\n",
    "\n",
    "# check size\n",
    "assert(df_svd.shape[0] == df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40716, 29)\n",
      "(40716, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd0</th>\n",
       "      <th>svd1</th>\n",
       "      <th>svd2</th>\n",
       "      <th>svd3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260.249839</td>\n",
       "      <td>0.740610</td>\n",
       "      <td>493.337727</td>\n",
       "      <td>-180.985846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97.839012</td>\n",
       "      <td>-1.970940</td>\n",
       "      <td>340.830035</td>\n",
       "      <td>-149.093792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106.380087</td>\n",
       "      <td>-2.712444</td>\n",
       "      <td>329.777569</td>\n",
       "      <td>-145.990369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112.361895</td>\n",
       "      <td>-1.728471</td>\n",
       "      <td>354.460670</td>\n",
       "      <td>-150.298124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3900.579864</td>\n",
       "      <td>59.762822</td>\n",
       "      <td>3878.367873</td>\n",
       "      <td>-802.341045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          svd0       svd1         svd2        svd3\n",
       "0   260.249839   0.740610   493.337727 -180.985846\n",
       "1    97.839012  -1.970940   340.830035 -149.093792\n",
       "2   106.380087  -2.712444   329.777569 -145.990369\n",
       "3   112.361895  -1.728471   354.460670 -150.298124\n",
       "4  3900.579864  59.762822  3878.367873 -802.341045"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svd['is_train'] = np.random.uniform(False, True, len(df_svd)) <= 0.3\n",
    "X_svd_train = \n",
    "X_svd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get rid of the cols that I suspect are not important\n",
    "X = df_all.copy().drop(drop, 1)\n",
    "\n",
    "# orig num comps\n",
    "ncomps = X.shape[1] - 1\n",
    "\n",
    "# run pca\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "pca = PCA(n_components=ncomps, whiten=True)\n",
    "pca_fit = pca.fit(X)\n",
    "\n",
    "# plot result        \n",
    "ax = pd.Series(pca_fit.explained_variance_ratio_.cumsum()).plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform X\n",
    "ncomps = 2\n",
    "X_pca = PCA(n_components=ncomps, whiten=True).fit_transform(X)\n",
    "\n",
    "# check transform\n",
    "print \"Mean: \", np.round(X_pca.mean(axis=0), decimals=5)\n",
    "print \"Stdv: \", np.round(X_pca.std(axis=0), decimals=5)\n",
    "print \"Corr: \", np.round(np.corrcoef(X_pca.T), decimals=5)\n",
    "\n",
    "# save pca to df and add back dropped cols\n",
    "df_pca = pd.DataFrame(X_pca,\n",
    "                      columns=['pca{}'.format(c) for c in range(ncomps)],\n",
    "                      index=df_all.index)\n",
    "for c in drop:\n",
    "    df_pca[c] = df_all[c]\n",
    "\n",
    "# save col names for easy filter later\n",
    "pcacols = [c for c in df_pca.columns if c[:3] == 'pca']\n",
    "\n",
    "# check size\n",
    "assert(df_pca.shape[0] == df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot('pca0', 'pca1',\n",
    "               data=df_pca,\n",
    "               fit_reg=False,\n",
    "               col=\"fault_severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_pca', df_pca)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look like linearly separable data (as exposed in correlation tasks above). Need to try kernel PCA to remove the nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform X\n",
    "X_kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=20).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save pca to df and add back dropped cols\n",
    "df_kpca = pd.DataFrame(X_kpca,\n",
    "                       columns=['kpca{}'.format(c) for c in range(ncomps)],\n",
    "                       index=df_all.index)\n",
    "\n",
    "for c in drop:\n",
    "    df_kpca[c] = df_all[c]\n",
    "\n",
    "# save col names for easy filter later\n",
    "kpcacols = [c for c in df_kpca.columns if c[:4] == 'kpca']\n",
    "\n",
    "# check size\n",
    "assert(df_kpca.shape[0] == df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot('kpca0', 'kpca1',\n",
    "               data=df_kpca,\n",
    "               fit_reg=False,\n",
    "               col=\"fault_severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_kpca', df_kpca)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### LLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "X_lle, err = manifold.locally_linear_embedding(X, n_neighbors=12,\n",
    "                                             n_components=2)\n",
    "print(\"Done. Reconstruction error: %g\" % err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save pca to df and add back dropped cols\n",
    "df_lle = pd.DataFrame(X_lle,\n",
    "                      columns=['lle{}'.format(c) for c in range(ncomps)],\n",
    "                      index=df_all.index)\n",
    "for c in drop:\n",
    "    df_lle[c] = df_all[c]\n",
    "\n",
    "# save col names for easy filter later\n",
    "llecols = [c for c in df_lle.columns if c[:3] == 'lle']\n",
    "\n",
    "# check size\n",
    "assert(df_lle.shape[0] == df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot('lle0', 'lle1',\n",
    "               data=df_lle,\n",
    "               fit_reg=False,\n",
    "               col=\"fault_severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_lle', df_lle)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried SVD and KPCA inputs: SVD outperformed any other. Using it as input to t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build tsne fitter\n",
    "from sklearn.manifold import TSNE\n",
    "#metric = metrics.pairwise.manhattan_distances\n",
    "tsne = TSNE(n_components=2,\n",
    "            #init='pca',\n",
    "            perplexity=50,\n",
    "            #metric=metric,\n",
    "            learning_rate=500,\n",
    "            method='barnes_hut',\n",
    "            verbose=2)\n",
    "\n",
    "# fit training data only\n",
    "Z = tsne.fit_transform(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to df\n",
    "df_tsne = pd.DataFrame(Z, columns=['x','y'], index=df_all.index)\n",
    "df_tsne['fault_severity'] = df_all.fault_severity\n",
    "\n",
    "# check size\n",
    "assert(df_tsne.shape[0] == df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kpca\n",
    "g = sns.lmplot('x', 'y',\n",
    "               data=df_tsne,\n",
    "               fit_reg=False,\n",
    "               col=\"fault_severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_tsne', df_tsne)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering may be interesting too: could add cluster membership as a new feature. \n",
    "\n",
    "* [Binary Jaccard similarity matrix fed into hierarchical cluster and then using the top \"nodes\"](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html)\n",
    "* [K-mode/median clustering](https://github.com/nicodv/kmodes)\n",
    "* [Cosine similarities fed into spectral clustering or dbscan](http://stackoverflow.com/questions/30089675/clustering-cosine-similarity-matrix)\n",
    "* Frequent itemset mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the log_feature features which are scaled volumes\n",
    "ecols = [c for c in df_all.columns if 'log_feature' not in c]\n",
    "X = df_all.copy()[ecols]\n",
    "X = X[(X.is_train == True)].drop(drop, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base similarity matrix\n",
    "similarity = np.dot(X, X.T)\n",
    "\n",
    "# squared magnitude of preference vectors (number of occurrences)\n",
    "square_mag = np.diag(similarity)\n",
    "\n",
    "# inverse squared magnitude\n",
    "inv_square_mag = 1 / square_mag\n",
    "\n",
    "# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\n",
    "inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
    "\n",
    "# inverse of the magnitude\n",
    "inv_mag = np.sqrt(inv_square_mag)\n",
    "\n",
    "# cosine similarity\n",
    "cosine = similarity * inv_mag\n",
    "cosine = cosine.T * inv_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find whole factors for matrix\n",
    "factors(cosine.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot it\n",
    "binning = 11\n",
    "plt.figure(figsize=(14, 11))\n",
    "cmap = sns.light_palette(\"blue\", as_cmap=True)\n",
    "plt.imshow(cosine[::binning, ::binning], interpolation='none', cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dbscan expects distances, so need 1-cosine input\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbclust = DBSCAN(min_samples=30, eps=0.6).fit_predict(1.0-cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# viz the cluster dist\n",
    "from collections import Counter\n",
    "for i in Counter(dbclust).items():\n",
    "    print i\n",
    "sns.distplot(dbclust, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### K-modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kmodes import kmodes\n",
    "\n",
    "# build the clusters\n",
    "km = kmodes.KModes(n_clusters=22, init='Cao', n_init=5, verbose=1)\n",
    "clusters = km.fit_predict(X)\n",
    "\n",
    "# viz the cluster dist\n",
    "for i in Counter(clusters).items():\n",
    "    print i\n",
    "sns.distplot(clusters, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not a terrible result, it somewhat mimics the tsne result, except for bins 3 and 4, as I don't see any isolated points in tsne, but it's not a 1:1 comparison. Let's see how this performs. First add the cluster assignments to the X matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "revisit the correlation measures\n",
    "run a hierarch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# create dist matrix\n",
    "Y = 1.0 - np.abs(df_svd.corr())\n",
    "Z = linkage(Y, 'complete', 'correlation')\n",
    "\n",
    "# check cophenetic coeffs; closer to 1, the better\n",
    "c, coph_dists = cophenet(Z, Y)\n",
    "print c\n",
    "\n",
    "# view dendro\n",
    "dendro = dendrogram(z, labels=Y.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "clusters = fcluster(Z, k, criterion='maxclust')\n",
    "plt.figure(figsize=(14, 11))\n",
    "plt.scatter(df_svd[:, 0], df_svd[:, 1], c=clusters, cmap='prism')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check validity of clusters using permutation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create permutation\n",
    "svm = SVC(kernel='linear')\n",
    "cv = StratifiedKFold(y, 2)\n",
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    svm, X, y, scoring=\"accuracy\", cv=cv, n_permutations=100, n_jobs=-1)\n",
    "print(\"Classification score %s (pvalue : %s)\" % (score, pvalue))\n",
    "\n",
    "# histo of permu scores\n",
    "plt.hist(permutation_scores, 20, label='Permutation scores')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score (pvalue %s)' % pvalue)\n",
    "plt.plot(2 * [1. / n_classes], ylim, '--k', linewidth=3, label='Luck')\n",
    "plt.ylim(ylim)\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append cluster membership to the no-location version of X\n",
    "a = np.array(clusters)[np.newaxis]\n",
    "print 'X no loc ', X_nl.shape\n",
    "X_nlc = np.concatenate((X_nl, a.T), axis=1)\n",
    "print 'X no loc+clu ', X_nlc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "X = \n",
    "y = \n",
    "\n",
    "# pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# univariate\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# combined estimator\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  features__univ_select__k=[1, 2],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# quick existing model test\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "thno.config.compute_test_value = 'warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rebuild the train and test sets\n",
    "xk_train = X_train.as_matrix()\n",
    "xk_test = X_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ohe = preprocessing.LabelBinarizer()\n",
    "yk_train = ohe.fit_transform(y_train)\n",
    "yk_test = ohe.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# common vars\n",
    "nb_epoch = 20\n",
    "dims = xk_train.shape[1]\n",
    "fs = list(factors(dims))\n",
    "batch_size = fs[-1]/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN model and layers\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(dims,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20167 samples, validate on 58970 samples\n",
      "Epoch 1/20\n",
      "20167/20167 [==============================] - 43s - loss: 7.4643 - acc: 0.5369 - val_loss: 7.0425 - val_acc: 0.5631\n",
      "Epoch 2/20\n",
      "20167/20167 [==============================] - 53s - loss: 7.4632 - acc: 0.5370 - val_loss: 7.0425 - val_acc: 0.5631\n",
      "Epoch 3/20\n",
      "20167/20167 [==============================] - 53s - loss: 7.4632 - acc: 0.5370 - val_loss: 7.0425 - val_acc: 0.5631\n",
      "Epoch 4/20\n",
      "20167/20167 [==============================] - 54s - loss: 7.4632 - acc: 0.5370 - val_loss: 7.0425 - val_acc: 0.5631\n",
      "Epoch 5/20\n",
      "20167/20167 [==============================] - 53s - loss: 7.4632 - acc: 0.5370 - val_loss: 7.0425 - val_acc: 0.5631\n",
      "Epoch 6/20\n",
      " 8106/20167 [===========>..................] - ETA: 29s - loss: 7.4108 - acc: 0.5402"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-cf87f897b24e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mshow_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           validation_data=(xg_test, yg_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(xk_train, yk_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          show_accuracy=True,\n",
    "          verbose=1,\n",
    "          validation_data=(xg_test, yg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.34\n",
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(xg_test, yg_test,\n",
    "                       show_accuracy=True,\n",
    "                       verbose=0)\n",
    "print('Loss: {:.2f}'.format(score[0]))\n",
    "print('Accuracy: {:.2f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/65 [=============>................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# output the highest probability class\n",
    "pred = model.predict_classes(xg_test)\n",
    "\n",
    "# check right / wrong\n",
    "right = np.nonzero(pred == yg_test)[0]\n",
    "wrong = np.nonzero(pred != yg_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predict_0</th>\n",
       "      <th>predict_1</th>\n",
       "      <th>predict_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.181694e-17</td>\n",
       "      <td>8.876193e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.550678e-15</td>\n",
       "      <td>1.301124e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.951803e-16</td>\n",
       "      <td>6.760162e-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.025460e-15</td>\n",
       "      <td>8.693152e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.590513e-18</td>\n",
       "      <td>1.239398e-38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  predict_0     predict_1     predict_2\n",
       "0   0          1  1.181694e-17  8.876193e-37\n",
       "1   1          1  2.550678e-15  1.301124e-32\n",
       "2   2          1  2.951803e-16  6.760162e-34\n",
       "3   3          1  4.025460e-15  8.693152e-32\n",
       "4  10          1  1.590513e-18  1.239398e-38"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base data\n",
    "ids = X_test.index\n",
    "y_probs = model.predict(xg_test)\n",
    "\n",
    "final = [[iden, (y_probs[j][0]), (y_probs[j][1]), (y_probs[j][2])] for j, iden in enumerate(ids)]\n",
    "frame = pd.DataFrame(final,columns=('id','predict_0','predict_1','predict_2'))\n",
    "frame.head()\n",
    "#frame.to_csv(\"keras.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MyXGBClassifier(n_rounds=100,\n",
    "                      eta=0.3,\n",
    "                      max_depth=10,\n",
    "                      subsample=0.5,\n",
    "                      colsample_bytree=0.9,\n",
    "                      booster='gbtree',\n",
    "                      objective='multi:softprob',\n",
    "                      num_class=n_classes,\n",
    "                      eval_metric='mlogloss',\n",
    "                      silent=0)\n",
    "\n",
    "param_grid = {'eta': [0.05, 0.3, 0.5],\n",
    "              'min_child_weight': [0.6, 0.8, 1.0],\n",
    "              'max_depth': [6, 8, 10],\n",
    "              'colsample_bytree': [0.3, 0.5],\n",
    "              'gamma': [0.0, 0.2, 0.4]\n",
    "             }\n",
    "\n",
    "gridcv = grid_search.GridSearchCV(clf, param_grid, n_jobs=-1, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-498b8ac3353b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgridcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0;31m# the results as we will raise the exception we got back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;31m# to the caller instead of returning any result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                     \u001b[0;31m# In case we had to terminate a managed pool, let\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# terminate does a join()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'multiprocessing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/pool.pyc\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/util.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    206\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 207\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining task handler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining result handler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    958\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.join(): timed out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0m_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgotit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mtrain = X_train.as_matrix()\n",
    "mlabels = y_train\n",
    "gridcv.fit(mtrain, mlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 36.259 (std: 26.086)\n",
      "Parameters: {'eta': 0.5, 'colsample_bytree': 0.5, 'max_depth': 8, 'gamma': 0.0, 'min_child_weight': 0.6}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 36.259 (std: 26.086)\n",
      "Parameters: {'eta': 0.5, 'colsample_bytree': 0.5, 'max_depth': 10, 'gamma': 0.0, 'min_child_weight': 0.6}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 35.984 (std: 25.801)\n",
      "Parameters: {'eta': 0.5, 'colsample_bytree': 0.5, 'max_depth': 6, 'gamma': 0.0, 'min_child_weight': 0.6}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 32.463 (std: 24.764)\n",
      "Parameters: {'eta': 0.3, 'colsample_bytree': 0.5, 'max_depth': 6, 'gamma': 0.0, 'min_child_weight': 0.6}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 32.463 (std: 24.764)\n",
      "Parameters: {'eta': 0.3, 'colsample_bytree': 0.5, 'max_depth': 8, 'gamma': 0.0, 'min_child_weight': 0.6}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   colsample_bytree  eta  gamma  max_depth  min_child_weight\n",
       "0               0.5  0.5      0          8               0.6\n",
       "1               0.5  0.5      0         10               0.6\n",
       "2               0.5  0.5      0          6               0.6\n",
       "3               0.5  0.3      0          6               0.6\n",
       "4               0.5  0.3      0          8               0.6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(gridcv.grid_scores_, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Algorithm Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Question: What are the important features in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "kf = cross_validation.KFold(len(y), n_folds=10, shuffle=True, random_state=4)\n",
    "y_pred = y.copy()\n",
    "\n",
    "# keep values for all features\n",
    "feat_impt = {}\n",
    "for f in features:\n",
    "    feat_impt[f] = []\n",
    "\n",
    "# go through all features    \n",
    "for train_index, test_index in kf:\n",
    "    clf = RF(n_estimators=50, min_samples_split=2, n_jobs=-1)\n",
    "    X_train, X_test = X.ix[train_index], X.ix[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred[test_index] = clf.predict(X_test)\n",
    "    importances = clf.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    for f in range(X.shape[1]):\n",
    "        feat_impt[features[indices[f]]].append(importances[indices[f]])\n",
    "\n",
    "for k in feat_impt:\n",
    "    val = np.mean(feat_impt[k])\n",
    "    std = np.std(feat_impt[k])\n",
    "    feat_impt[k] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort the features by importance\n",
    "sorted_feat = sorted(feat_impt.items(), key=operator.itemgetter(1))\n",
    "sorted_feat.reverse()\n",
    "\n",
    "# bookkeeping\n",
    "cumper = 0.\n",
    "nbr = False\n",
    "\n",
    "# make a copy of original dataframe to drop unneeded features\n",
    "X_keep = X.copy()\n",
    "\n",
    "# print out importances\n",
    "print \"item -- feature -- weight -- cumm_weight\"\n",
    "for i, k in enumerate(sorted_feat):\n",
    "    cumper += k[1]\n",
    "    \n",
    "    # tell me when we hit 90% of weights\n",
    "    if (nbr is False and cumper >= 0.95):\n",
    "        print '**----  Reached 95%  ----**'\n",
    "        nbr = True\n",
    "    print i, k[0], '{:.1%}'.format(k[1]), '{:.0%}'.format(cumper)\n",
    "    \n",
    "    # drop unneeded features\n",
    "    if nbr is True:\n",
    "        X_keep = X_keep.drop(k[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_keep = X_keep.columns\n",
    "\n",
    "# checks\n",
    "print \"\\n%d observations of %d features\" % X_keep.shape\n",
    "print \"Unique labels:\", np.unique(y)\n",
    "print \"Gut-check features: %i\" % len(features_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the ML test harness for RF only and see if the model improves (below is cut and paste of earlier code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ml_models = []\n",
    "ml_models.append([\"Random Forest\", RF, {'n_estimators':72,\n",
    "                                        'min_samples_split':9,\n",
    "                                        'n_jobs':-1}])\n",
    "acc = []\n",
    "prec = []\n",
    "recl = []\n",
    "f1 = []\n",
    "cms = []\n",
    "for m in ml_models:\n",
    "    y_pred, mean_acc, mean_prec, mean_recl, mean_f1 = run_cv(X_keep, y, m[1], **m[2])\n",
    "    acc.append(mean_acc)\n",
    "    prec.append(mean_prec)\n",
    "    recl.append(mean_recl)\n",
    "    f1.append(mean_f1)\n",
    "    cms.append((m[0], metrics.confusion_matrix(y, y_pred)))\n",
    "\n",
    "draw_confusion_matrices(cms, np.unique(y))\n",
    "max_idx, max_value = max(enumerate(acc), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best accuracy:', ml_models[max_idx][0], acc[max_idx])\n",
    "max_idx, max_value = max(enumerate(prec), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best precision:', ml_models[max_idx][0], prec[max_idx])\n",
    "max_idx, max_value = max(enumerate(recl), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best recall:', ml_models[max_idx][0], recl[max_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RF predictive power got worse when features contributing < 10% weight were dropped.\n",
    "* **RF predictive power got better when features contributing < 5% weight were dropped.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Question: What model has best calibration and discrimination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration measures the difference between actual and predicted probability for individual groups. Discrimination measures the difference between model predictions and the baseline probability. Equations are taken from [Yang, Yates, and Smith (1991)](http://psychology.huji.ac.il/.upload/Ilan/YanivYatesSmith1991PB.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_prob_cv(X, y, clf_class, **kwargs):\n",
    "    kf = cross_validation.KFold(len(y), n_folds=10, shuffle=True)\n",
    "    y_prob = np.zeros((len(y), 3))\n",
    "    for train_index, test_index in kf:\n",
    "        clf = clf_class(**kwargs)\n",
    "        X_train, X_test = X.ix[train_index], X.ix[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "    return y_prob\n",
    "\n",
    "def calibration(prob, outcome,n_bins=10):\n",
    "    prob = np.array(prob)\n",
    "    outcome = np.array(outcome)\n",
    "    c = 0.0\n",
    "    judgement_bins = np.arange(n_bins + 1) / n_bins\n",
    "    bin_num = np.digitize(prob,judgement_bins)\n",
    "    for j_bin in np.unique(bin_num):\n",
    "        in_bin = bin_num == j_bin\n",
    "        predicted_prob = np.mean(prob[in_bin])\n",
    "        true_bin_prob = np.mean(outcome[in_bin])\n",
    "        c += np.sum(in_bin) * ((predicted_prob - true_bin_prob) ** 2)\n",
    "    return c / len(prob)\n",
    "\n",
    "def discrimination(prob, outcome, n_bins=10):\n",
    "    prob = np.array(prob)\n",
    "    outcome = np.array(outcome)\n",
    "    d = 0.0\n",
    "    base_prob = np.mean(outcome)\n",
    "    judgement_bins = np.arange(n_bins + 1) / n_bins\n",
    "    bin_num = np.digitize(prob,judgement_bins)\n",
    "    for j_bin in np.unique(bin_num):\n",
    "        in_bin = bin_num == j_bin\n",
    "        true_bin_prob = np.mean(outcome[in_bin])\n",
    "        d += np.sum(in_bin) * ((true_bin_prob - base_prob) ** 2)\n",
    "    return d / len(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Lower calibration and higher discrimination are preferred...\"\n",
    "\n",
    "ml_models = []\n",
    "#ml_models.append([\"XGBoost\", XGBC, {'max_depth':3, 'n_estimators':300, 'learning_rate':0.05}])\n",
    "#ml_models.append([\"Support Vector Machine\", SVC, {}])\n",
    "ml_models.append([\"Linear Logistic Regression\", LR, {'multi_class':'multinomial',\n",
    "                                                     'solver':'lbfgs'}])\n",
    "ml_models.append([\"Naive Bayes\", NB, {}])\n",
    "ml_models.append([\"Decision Trees\", DT, {}])\n",
    "ml_models.append([\"Random Forest\", RF, {'n_estimators':200,\n",
    "                                        'min_samples_split':2, 'n_jobs':-1}])\n",
    "\n",
    "cal_err = []\n",
    "discrim = []\n",
    "for m in ml_models:\n",
    "    print \"\\n\"+m[0]\n",
    "    pred_prob = run_prob_cv(X, y, m[1], **m[2])\n",
    "    churn_prob, is_churn = pred_prob[:,1], y == 1\n",
    "    cal_err.append(calibration(churn_prob, is_churn))\n",
    "    discrim.append(discrimination(churn_prob,is_churn))\n",
    "    print '{:20} {:.4f}'.format(\"Calibration Error\", cal_err[-1])\n",
    "    print '{:20} {:.4f}'.format(\"Discrimination\", discrim[-1])\n",
    "    \n",
    "idx, value = min(enumerate(cal_err), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.3%}'.format('\\nBest calibration error:', ml_models[idx][0], cal_err[idx])\n",
    "idx, value = max(enumerate(discrim), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best discrimination:', ml_models[idx][0], discrim[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Question: Can we tune the model params to get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RF(n_jobs=-1)\n",
    "param_grid = {\"n_estimators\":[50, 75, 100],\n",
    "              \"max_depth\": [None, 3],\n",
    "              \"max_features\": [None, 'auto'],\n",
    "              \"min_samples_split\": [4, 9],\n",
    "              \"min_samples_leaf\": [1],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "gs = grid_search.GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "gs.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(gs.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best scores using X (took 542.40 seconds for 32 candidate parameter settings):**\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.956 (std: 0.007)\n",
    "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 500, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.956 (std: 0.009)\n",
    "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 1000, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: 0.955 (std: 0.006)\n",
    "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 1000, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "\n",
    "**Best scores using X_keep (took 471.68 seconds for 48 candidate parameter settings):**\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.957 (std: 0.007)\n",
    "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 1000, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.956 (std: 0.006)\n",
    "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 500, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: 0.956 (std: 0.007)\n",
    "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 50, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_models = []\n",
    "ml_models.append([\"Random Forest\", RF, {'bootstrap': True,\n",
    "                                        'min_samples_leaf': 1,\n",
    "                                        'n_estimators': 500,\n",
    "                                        'min_samples_split': 4,\n",
    "                                        'criterion': 'entropy',\n",
    "                                        'max_features': 'auto',\n",
    "                                        'max_depth': None,\n",
    "                                        'n_jobs': -1}])\n",
    "acc = []\n",
    "prec = []\n",
    "recl = []\n",
    "f1 = []\n",
    "cms = []\n",
    "for m in ml_models:\n",
    "    y_pred, mean_acc, mean_prec, mean_recl, mean_f1 = run_cv(X, y, m[1], **m[2])\n",
    "    acc.append(mean_acc)\n",
    "    prec.append(mean_prec)\n",
    "    recl.append(mean_recl)\n",
    "    f1.append(mean_f1)\n",
    "    cms.append((m[0], metrics.confusion_matrix(y, y_pred)))\n",
    "draw_confusion_matrices(cms, np.unique(y))\n",
    "max_idx, max_value = max(enumerate(acc), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best accuracy:', ml_models[max_idx][0], acc[max_idx])\n",
    "max_idx, max_value = max(enumerate(prec), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best precision:', ml_models[max_idx][0], prec[max_idx])\n",
    "max_idx, max_value = max(enumerate(recl), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best recall:', ml_models[max_idx][0], recl[max_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is hardly better than the initial guess, but it's technically simpler, so I'll run with this one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use 10 estimators so predictions are all multiples of 0.1\n",
    "m = ([\"Random Forest\", RF, {'bootstrap': True,\n",
    "                            'min_samples_leaf': 1,\n",
    "                            'n_estimators': 10,\n",
    "                            'min_samples_split': 1,\n",
    "                            'criterion': 'entropy',\n",
    "                            'max_features': 'auto',\n",
    "                            'max_depth': None,\n",
    "                            'n_jobs': -1}])\n",
    "pred_prob = run_prob_cv(X, y, m[1], **m[2])\n",
    "pred_churn = pred_prob[:,1]\n",
    "is_churn = y == 1\n",
    "\n",
    "# Number of times a predicted probability is assigned to an observation\n",
    "counts = pd.value_counts(pred_churn)\n",
    "\n",
    "# calculate true probabilities\n",
    "true_prob = {}\n",
    "for prob in counts.index:\n",
    "    true_prob[prob] = np.mean(is_churn[pred_churn == prob])\n",
    "    true_prob = pd.Series(true_prob)\n",
    "\n",
    "# pandas-fu\n",
    "counts = pd.concat([counts,true_prob], axis=1).reset_index()\n",
    "counts.columns = ['pred_prob', 'count', 'true_prob']\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "baseline = np.mean(is_churn)\n",
    "ggplot(counts, aes(x='pred_prob',y='true_prob',size='count')) + \\\n",
    "        geom_point(color='blue') + \\\n",
    "        stat_function(fun = lambda x: x, color='red') + \\\n",
    "        stat_function(fun = lambda x: baseline, color='green') + \\\n",
    "        xlim(-0.05,  1.05) + \\\n",
    "        ylim(-0.05,1.05) + \\\n",
    "        ggtitle(\"Random Forest\") + \\\n",
    "        xlab(\"Predicted probability\") + ylab(\"Relative frequency of outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# log_feature_rarityThreshold <- 50\n",
    "# event_type_rarity_threshold <- 5\n",
    "\n",
    "# log_feature[, \":=\"(numlf = makeNumeric(log_feature), log_feature = makeReadable(log_feature))]\n",
    "# event_type[, \":=\"(numet = makeNumeric(event_type), event_type = makeReadable(event_type))]\n",
    "# resource_type[, \":=\"(resource_type = makeReadable(resource_type))]\n",
    "# severity_type[, \":=\"(numst = makeNumeric(severity_type), severity_type = makeReadable(severity_type))]\n",
    "\n",
    "\n",
    "# train[, \":=\"(numloc = makeNumeric(location),\n",
    "#              location = makeReadable(location))]\n",
    "# test[, \":=\"(numloc = makeNumeric(location),\n",
    "#             location = makeReadable(location), fault_severity = -1)]\n",
    "\n",
    "# total <- rbind(train, test)%>%setkey(\"id\")\n",
    "\n",
    "# if(groupCardinalities){\n",
    "#   #LF reduction\n",
    "#   rare_lf <- log_feature[total][,.N, by=log_feature][N<=log_feature_rarityThreshold][,log_feature]\n",
    "#   log_feature$log_feature[log_feature$log_feature%in%rare_lf] <- \"rare_lf\"\n",
    "  \n",
    "#   rare_et <- event_type[total][,.N, by=event_type][N<=event_type_rarity_threshold][,event_type]\n",
    "#   event_type$event_type[event_type$event_type%in%rare_et] <- \"rare_et\"\n",
    "#   rm(list=c(\"rare_lf\", \"rare_et\"))\n",
    "# }\n",
    "\n",
    "# if(computeFeatures){\n",
    "\n",
    "#   t1 <- log_feature[total][,.(\n",
    "#     loc_nid = uniqueN(id),\n",
    "#     loc_nlf = uniqueN(log_feature),\n",
    "#     loc_sumvol = sum(volume),\n",
    "#     loc_avgvol = mean(volume),\n",
    "#     loc_sdvol = ifelse(is.na(sd(volume)), 0, sd(volume)),\n",
    "#     loc_logvol = log(sum(volume)),\n",
    "#     loc_vollog = sum(log(volume+1)),\n",
    "#     loc_sqrtvol = sqrt(sum(volume)),\n",
    "#     loc_volsqrt = sum(sqrt(volume))), keyby=location]\n",
    "  \n",
    "#   t2 <- resource_type[total][, .(loc_nrt = uniqueN(resource_type)), keyby=location]\n",
    "#   t3 <- event_type[total][, .(loc_net = uniqueN(event_type)), keyby=location]\n",
    "#   t4 <- severity_type[total][, .(loc_nst = uniqueN(severity_type)), keyby=location]\n",
    "  \n",
    "#   joined_total <- resource_type[event_type][severity_type][total]\n",
    "#   # nombres de combinaisons différentes de resource_type/event_type par location\n",
    "#   t5 <- joined_total[,etrtcomb := paste(resource_type, event_type, sep = \"x\")][, .(loc_etrtcomb = uniqueN(etrtcomb)), keyby=location]\n",
    "#   t6 <- joined_total[,etstcomb := paste(severity_type, event_type, sep = \"x\")][, .(loc_etstcomb = uniqueN(etstcomb)), keyby=location]\n",
    "#   t7 <- joined_total[,rtstcomb := paste(severity_type, resource_type, sep = \"x\")][, .(loc_rtstcomb = uniqueN(rtstcomb)), keyby=location]\n",
    "  \n",
    "#   location_info_total <- t1[t2][t3][t4][t5][t6][t7]\n",
    "  \n",
    "#   lf_info <- log_feature[total][,.(\n",
    "#     sumlf = sum(numlf), \n",
    "#     avglf = mean(numlf), \n",
    "#     sdlf = ifelse(is.na(sd(numlf)), 0, sd(numlf)),\n",
    "#     minlf = min(numlf),\n",
    "#     maxlf = max(numlf)),by=id]\n",
    "  \n",
    "#   etst <- event_type[, .(\n",
    "#     id, \n",
    "#     minumet = min(numet), \n",
    "#     maxnumet = max(numet), \n",
    "#     avgnumet = mean(numet)), by=id][severity_type][total][,.(et.st = avgnumet*numst),by=id]\n",
    "  \n",
    "#   total <- total[lf_info][etst]\n",
    "#   total <- merge(total, location_info_total, by = \"location\")\n",
    "# }\n",
    "# setkeyv(total[,\":=\"(location=NULL)], c(\"id\", \"fault_severity\"))\n",
    "\n",
    "# na.ids <- log_feature[total][fault_severity != -1][is.na(log_feature)][,id]\n",
    "# if(length(na.ids) > 0)\n",
    "#   total <- total[-na.ids]\n",
    "\n",
    "# total_lf_volume  <- dcast(\n",
    "#   log_feature[total],\n",
    "#   id + fault_severity ~ log_feature,\n",
    "#   value.var = \"volume\",\n",
    "#   fun = sum\n",
    "# )\n",
    "\n",
    "# total_et <-\n",
    "#   dcast(event_type[total], id + fault_severity ~ event_type, value.var = \"event_type\", fun = length)\n",
    "# total_rt <-\n",
    "#   dcast(\n",
    "#     resource_type[total], id + fault_severity ~ resource_type, value.var = \"resource_type\", fun = length\n",
    "#   )\n",
    "# total_st <-\n",
    "#   dcast(\n",
    "#     severity_type[total], id + fault_severity ~ severity_type, value.var = \"severity_type\", fun = length\n",
    "#   )\n",
    "\n",
    "# total.wide <-\n",
    "#   total[total_lf_volume][total_et][total_rt][total_st]\n",
    "\n",
    "# if(computeManualInteractions){\n",
    "#   total.wide[,\":=\"(\n",
    "\n",
    "#     som_vol_test = (f203 * f312 * f232 * f170),\n",
    "    \n",
    "#     som_vol_feat = (f82 + f203 + f71 + f193 + f80),\n",
    "#     som_vol_feat_c0 = (f313 + f233 + f315),\n",
    "#     som_vol_feat_c1 = (f82 + f203 + f170),\n",
    "#     som_vol_feat_c2 = (f71 + f193 + f80) ) ]\n",
    "# }\n",
    "\n",
    "# train.wide <- total.wide[fault_severity != -1,-\"id\", with = FALSE]\n",
    "# test.wide <- total.wide[fault_severity == -1,-\"id\", with = FALSE]\n",
    "\n",
    "# if(writeFiles){\n",
    "#   writeLines(\"Writing train.csv and test.csv...\")\n",
    "#   write.csv(train.wide, paste(sep = \"-\", \"train.csv\"), row.names = F, quote = F)\n",
    "#   write.csv(test.wide[,.SD, .SDcols = -\"fault_severity\"], paste(sep = \"-\", \"test.csv\"), row.names = F, quote = F)\n",
    "#   writeLines(\"...done\")}\n",
    "\n",
    "# xtrain <- as.matrix(train.wide[,-\"fault_severity\", with = F])\n",
    "# names(xtrain) <- setdiff(names(train.wide), \"fault_severity\")\n",
    "# ytrain <- train.wide$fault_severity\n",
    "\n",
    "# xtest <-  as.matrix(test.wide[,-\"fault_severity\", with = F])\n",
    "# names(xtest) <- setdiff(names(test.wide), \"fault_severity\")\n",
    "# test.id <- test$id\n",
    "\n",
    "# writeLines(\"Cleaning up...\")\n",
    "# rm(list=c(\"na.ids\", \"total.wide\", \"total_et\", \"total_rt\", \"total_st\", \n",
    "#           \"total_lf_volume\", \"train.wide\", \"test.wide\", \"total\",\n",
    "#           \"location_info_total\",\"joined_total\", \"lf_info\", paste0(\"t\", 1:7)))"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "notes.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
